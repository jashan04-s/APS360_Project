# -*- coding: utf-8 -*-
"""APS360_Baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PxpT1nxPWh4yKmUnQCy91me1MoY9xgO3
"""

import math
import torch.nn as nn
import numpy as np
import time
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import TensorDataset, DataLoader
from PIL import Image
import matplotlib.pyplot as plt
from torch.utils.data import Dataset
import torch.nn as nn
import torch.nn.functional as F
import cv2



def display(img):
    plt.figure()
    plt.set_cmap('gray')
    plt.imshow(img)
    plt.show()

def display_colored(img):
    plt.figure()
    plt.imshow(img)
    plt.show()

def rgb_image(l, ab):
    shape = (l.shape[0],l.shape[1],3)
    img = np.zeros(shape)
    img[:,:,0] = l[:,:]
    img[:,:,1:]= ab
    img = img.astype('uint8')
    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    return img

def rgb_image_A(l, ab):
    shape = (l.shape[0],l.shape[1],3)
    img = np.ones(shape)

    img[:,:,1]= ab[:,:,0]
    img[:,:,0] = np.ones((224,224)) * 128

    print("The minimum A is", min(ab[:,:,0].flatten()))
    print("The maximum A is", max(ab[:,:,0].flatten()))
    img = img.astype('uint8')
    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    return img

def rgb_image_B(l, ab):
    shape = (l.shape[0],l.shape[1],3)

    img = np.zeros(shape)
    img[:,:,2]= ab[:,:,1]
    img[:,:,0] = np.ones((224,224)) * 128

    print("The minimum B is", min(ab[:,:,1].flatten()))
    print("The maximum B is", max(ab[:,:,1].flatten()))
    img = img.astype('uint8')
    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)
    return img

class LABImageDataset(Dataset):
    def __init__(self, L_data, ab_data):
        self.L_data = L_data
        self.ab_data = ab_data

    def __len__(self):
        return len(self.L_data)

    def __getitem__(self, idx):
        L = self.L_data[idx]
        ab = self.ab_data[idx]
        return L, ab

def get_data_loader(batch_size = 64):


    # npArray_ab1 = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/ab/ab/ab1.npy")
    # npArray_ab2 = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/ab/ab/ab2.npy")
    # npArray_ab3 = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/ab/ab/ab3.npy")
    # npArray_l = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/l/gray_scale.npy")
    # npArray_ab3 = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/ab/ab/ab3.npy")
    # npArray_l = np.load("/content/drive/MyDrive/Image Colorization/Image Colorization/l/gray_scale.npy")

    npArray_l = np.load("gray_scale.npy")
    npArray_ab1 = np.load("ab/ab1.npy")
    npArray_ab2 = np.load("ab/ab2.npy")
    npArray_ab3 = np.load("ab/ab3.npy")

    npArray_l = npArray_l[:10000]
    # npArray_ab = np.concatenate((npArray_ab1, npArray_ab2, npArray_ab3))
    npArray_ab = npArray_ab1
    # print(npArray_l[0])
    # print(npArray_ab[0])
    # print(npArray_ab[0][:][:][1:].shape)
    # display(npArray_l[0])
    # display_colored(rgb_image(npArray_l[0], npArray_ab[0]))

    # npArray_ab = npArray_ab[:1000]
    # npArray_ab = npArray_ab.swapaxes(2, 3).swapaxes(1, 2).swapaxes(0, 1)
    # npArray_l = npArray_l[:1000]
    # new_shape = (1000, 224, 224,1)
    # npArray_l = npArray_l.reshape(new_shape)

    lab_dataset = LABImageDataset(npArray_l, npArray_ab)
    print(npArray_ab.shape)
    # print(npArray_ab1.shape)
    # print(npArray_ab2.shape)
    # print(npArray_ab3.shape)
    print(npArray_l.shape)
    relevant_indices = np.arange(1, 10000)
    # relevant_indices = np.arange(1, 5000)
    #relevant_indices = np.arange(1, 10000)

    print(lab_dataset)

    np.random.seed(1000)

    np.random.shuffle(relevant_indices)

    split = int(len(relevant_indices) * 0.8)

    split_in_val_test = split + int(len(relevant_indices) * 0.1)

    train_indices = relevant_indices[:split]
    val_indices = relevant_indices[split: split_in_val_test]
    test_indices = relevant_indices[split_in_val_test:]

    # np.random.shuffle(train_indices)
    # np.random.shuffle(test_indices)
    # np.random.shuffle(val_indices)

    train_sampler = SubsetRandomSampler(train_indices)
    val_sampler = SubsetRandomSampler(val_indices)
    test_sampler = SubsetRandomSampler(test_indices)

    train_loader = DataLoader(lab_dataset, batch_size = batch_size, sampler = train_sampler)
    val_loader = DataLoader(lab_dataset, batch_size = batch_size, sampler = val_sampler)
    test_loader = DataLoader(lab_dataset, batch_size = batch_size, sampler = test_sampler)

    return train_loader, val_loader, test_loader

train_loader, val_loader, test_loader = get_data_loader(64)
'''
for images, labels in train_loader:
   display(images[:][0])
   display_colored(rgb_image(images[:][0], labels[:][0]))
   display_colored(rgb_image_A(images[:][0], labels[:][0]))
   display_colored(rgb_image_B(images[:][0], labels[:][0]))
   break
'''
"""
class Generator(nn.Module):
  def __init__(self):
    super(Generator, self).__init__()
    self.encoder = nn.Sequential(
      nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),
      nn.ReLU(inplace=True),
      nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
      nn.ReLU(inplace=True),
      nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
      nn.ReLU(inplace=True),
    )
    self.decoder = nn.Sequential(
      nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
      nn.ReLU(inplace=True),
      nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
      nn.ReLU(inplace=True),
      nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1),
      # nn.ReLU(),
      # nn.Tanh(),
    )

  def forward(self, x):
    x = self.encoder(x.unsqueeze(1))
    x = self.decoder(x)
    return x
    # return x.permute(0, 2, 3, 1) '''
"""
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.ReLU(inplace=True),
            nn.ConvTranspose2d(64, 2, kernel_size=4, stride=2, padding=1),
            nn.ReLU(),
            # nn.Tanh(),
        )

    def forward(self, x):
        x = self.encoder(x.unsqueeze(1))
        x = self.decoder(x)
        return x

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 128, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2, inplace=True),
        )

        self.fcLayer = nn.Sequential(
            nn.Linear(200704,1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.model(x)
        x = x.view(x.size(0),-1)
        x = self.fcLayer(x)
        return x

'''
for images, labels in train_loader:
    print("Input shape:", images.shape)
    print("Labels shape:", labels.shape)
    break
'''
print(torch.cuda.is_available())
generator = Generator()
discriminator = Discriminator()

generator.cuda()
discriminator.cuda()

L1_criterion = nn.L1Loss()
bc_criterion = nn.BCELoss()
mse_criterion = nn.MSELoss()

optimizer_generator = torch.optim.Adam(generator.parameters(), lr=0.001)
optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=0.001)

# Training loop
num_epochs = 20

current_image_grayscale = 0
generated_color_image = 0
real_color_image = 0
testOutput =0


for epoch in range(num_epochs):
    start = time.time()
    g_train_loss = 0.0
    d_train_loss = 0.0
    batch = 0 
    for data in train_loader:

        images, labels = data
        current_image_grayscale = images
        real_color_image = labels

        images = images.float()
        labels = labels.float()

        images = images.cuda()
        labels = labels.cuda()

        # Adversarial ground truths
        
        valid = torch.ones(images.size()[0], 1) # real label
        fake = torch.zeros(images.size()[0], 1) # fake label
        
        valid = valid.cuda()
        fake = fake.cuda() 

        optimizer_generator.zero_grad()
        optimizer_discriminator.zero_grad()

        noise = images
        noise = noise.cuda()

        fake_images = generator(noise)
       
        noise = noise[:, None, :, :]   
        
        complete_fake_image = torch.cat([noise, fake_images], dim = 1)
        complete_fake_image = complete_fake_image.cuda()
        

        grayscale = images[:, :, :, None]

        complete_real_image = torch.cat([grayscale, labels], dim = 3)
        
        complete_real_image = complete_real_image.reshape((math.ceil(complete_real_image.numel()/(3*224*224)),3,224,224))
        complete_real_image = complete_real_image.cuda()

        discriminator_images = torch.cat([complete_real_image, complete_fake_image])
        discriminator_labels = torch.cat([valid, fake])
        
        discriminator_images = discriminator_images.cuda()
        discriminator_labels = discriminator_labels.cuda()


        d_loss = bc_criterion(discriminator(discriminator_images), discriminator_labels)

        print(d_loss)
        d_loss.backward()
        optimizer_discriminator.step()
        
        d_train_loss += d_loss.item()

        generator_loss = bc_criterion(discriminator(complete_fake_image.detach()), valid)
        
        print(generator_loss)
        g_train_loss += generator_loss.item()
        generator_loss.backward()
        optimizer_generator.step()
        
        testOutput = generator(images)
        testOutput = testOutput.cpu().detach().numpy()
        print(testOutput.shape)
        testOutput = np.transpose(testOutput,(0,2,3,1))
        print(testOutput.shape)
        
        if(batch%10 == 0):
            display(current_image_grayscale[0])
            display_colored(rgb_image(current_image_grayscale[0], real_color_image[0]))
            display_colored(rgb_image(current_image_grayscale[0], testOutput[0]))
            display_colored(rgb_image_A(current_image_grayscale[0], real_color_image[0]))
            display_colored(rgb_image_A(current_image_grayscale[0], testOutput[0]))
            display_colored(rgb_image_B(current_image_grayscale[0], real_color_image[0]))
            display_colored(rgb_image_B(current_image_grayscale[0], testOutput[0]))

        print(batch)
        batch+=1
        """
        # Train Generator
        optimizer_generator.zero_grad()
        g_outputs = generator(images)
        g_outputs = g_outputs.float()
        # g_outputs = (g_outputs + 1) * 127.5

        labels = labels.permute(0, 3, 1, 2)
        # labels = labels.squeeze(dim=3)

        # print(images.shape)
        # print(g_outputs.shape)
        # print(labels.shape)

        # print(g_outputs.shape)
        # print(discriminator(g_outputs).shape)
        # print(valid.shape)

        g_outputs = g_outputs.cuda()
        valid = valid.cuda()
        fake = fake.cuda()

        # g_loss = mse_criterion(discriminator(g_outputs), valid)
        g_loss = L1_criterion(g_outputs, labels)
        # g_loss = -torch.mean(torch.log(torch.sigmoid(discriminator(g_outputs)) + 1e-8))  # Adversarial loss
        # pixel_loss = mse_criterion(g_outputs, labels)
        # g_loss = 1.0 * g_loss + pixel_loss

        g_loss = g_loss.float()
        g_outputs = g_outputs.float()

        g_loss.backward()
        optimizer_generator.step()
        # g_train_loss += g_loss.item()*images.size(0)
        g_train_loss += g_loss.item()
        # g_outputs.cpu()

        generated_color_image = g_outputs
        # Train Discriminator
        # labels = np.transpose()
        # labels = np.transpose(labels, (0, 3, 1, 2))
        # discriminator(labels)
        optimizer_discriminator.zero_grad()
        real_loss = bc_criterion(discriminator(labels), valid)
        fake_loss = bc_criterion(discriminator(g_outputs.detach()), fake)task
        d_loss = (real_loss + fake_loss) / 2
        d_loss.backward()
        optimizer_discriminator.step()
        d_train_loss += d_loss.item()
        """
    generated_color_image = generated_color_image.cpu().detach().numpy()
    g_train_loss = g_train_loss/len(train_loader)
    d_train_loss = d_train_loss/len(train_loader)
    print('Epoch: {} \tG Loss: {:.6f}\tD Loss: {:.6f}'.format(epoch, g_train_loss, d_train_loss))
    print("Time Taken: ", time.time() - start)

    
    
    
    # print(real_color_image[0])
    # print("--------------------")
    # print(generated_color_image[0])


